The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. The bestperforming models connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. We show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
The Transformer allows for significantly more parallelization and can reach a new state of the art in translating quality. It can be trained for as little as twelve hours on eight P100 GPUs. It is the first transduction model relying entirely on self-attention to compute representations of its input and output.
We call our particular attention "Scaled Dot-Product Attention" (Figure 2) The input consists of queries and keys of dimension dk, and values ofdimension dv. We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values.
In this section we compare various aspects of self-attention layers to the recurrent layers commonly used for mapping one variable-length sequence to another. We use learned embeddings to convert the input and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-for-mation and softmax function to convert decoder output to predicted next-token probabilities.
The shorter these paths between any combination of positions in the inputand output sequences, the easier it is to learn long-range dependencies. A self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n)sequential operations. We train our models on one machine with 8 NVIDIA P100 GPUs. For our base models, each training step took about 0.4 seconds.
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer) outperforms the best previously reported models (including ensembles) by more than 2.0. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs.
We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences. We used beam search as described in the previous section, but no checkpoint averaging. The Transformer generalizes well to English constituency parsing.
The Transformer is the first sequence transduction model based entirely on attention. It can be trained significantly faster than architectures based on recurrent or convolutional layers. We are excited about the future of attention-based models and plan to apply them to other tasks. We also plan to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images and video.
, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. Can active memory replace attention? In Advances in Neural. Information Processing Systems, (NIPS), 2016. In International Conference. on Learning Representations (ICLR) - 2016.
The attention mechanism following long-distance dependencies in the                encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘ making...more difficult’ Attentions here shown only for the word ‘Making’.
A look at some of the best and worst of the season. A look at a few of the most memorable moments from the season so far. A glimpse into the future. A peek into the past. A taste of the present. And a taste of future.