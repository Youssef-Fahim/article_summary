{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i0545652/article_summary/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i0545652/article_summary/.venv/lib/python3.8/site-packages/datasets/load.py:1429: FutureWarning: The repository for scientific_papers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scientific_papers\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('scientific_papers', 'arxiv')\n",
    "\n",
    "# Get the training, validation and test datasets\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:10<00:00, 23.28 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:01<00:00, 19.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "model_path = \"facebook/bart-large-cnn\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "\n",
    "max_input_length = 1024\n",
    "max_output_length = 128\n",
    "batch_size = 2\n",
    "\n",
    "def process_data_to_model_inputs(batch):    \n",
    "    # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "    outputs = tokenizer(batch[\"abstract\"], padding=\"max_length\", truncation=True, max_length=max_output_length)\n",
    "    \n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # ignore the PAD token\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in\n",
    "                       batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.select(range(250))\n",
    "val_dataset = val_dataset.select(range(25))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
    ")\n",
    "\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "task_dataset = train_dataset\n",
    "for i in range(len(task_dataset)):\n",
    "    ls = task_dataset[i][\"input_ids\"].numpy()\n",
    "    indices = np.where(ls == 2)\n",
    "    if len(indices) > 1:\n",
    "        print(i)\n",
    "        print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, gradient_checkpointing=True, use_cache=False)\n",
    "#model = BartForConditionalGeneration.from_pretrained(model_path, gradient_checkpointing=True, use_cache=False)\n",
    "\n",
    "# set hyperparameters\n",
    "model.config.num_beams = 2\n",
    "model.config.max_length = 128\n",
    "model.config.min_length = 80\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/p134q1115s35c2fqxthfm4_c0000gn/T/ipykernel_52876/1956387722.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "/Users/i0545652/article_summary/.venv/lib/python3.8/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids==-100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"] \n",
    "    )[\"rouge2\"].mid \n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #fp16=True,\n",
    "    use_mps_device=True,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=5,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i0545652/article_summary/.venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 250\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 31\n",
      "  Number of trainable parameters = 406290432\n",
      " 16%|█▌        | 5/31 [00:29<02:20,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5405, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10/31 [00:54<01:49,  5.21s/it]***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9054, 'learning_rate': 3.387096774193548e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i0545652/article_summary/.venv/lib/python3.8/site-packages/transformers/generation_utils.py:2797: UserWarning: The operator 'aten::remainder.Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  next_tokens = next_tokens % vocab_size\n",
      "                                               \n",
      " 32%|███▏      | 10/31 [07:32<01:49,  5.21s/it]Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8460090160369873, 'eval_rouge2_precision': 0.1289, 'eval_rouge2_recall': 0.0981, 'eval_rouge2_fmeasure': 0.1087, 'eval_runtime': 397.5472, 'eval_samples_per_second': 0.063, 'eval_steps_per_second': 0.033, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10/special_tokens_map.json\n",
      " 48%|████▊     | 15/31 [08:28<09:41, 36.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8904, 'learning_rate': 2.5806451612903226e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [08:57<01:59, 10.86s/it]***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6771, 'learning_rate': 1.774193548387097e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 65%|██████▍   | 20/31 [15:55<01:59, 10.86s/it]Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6968960762023926, 'eval_rouge2_precision': 0.1231, 'eval_rouge2_recall': 0.1244, 'eval_rouge2_fmeasure': 0.1227, 'eval_runtime': 418.3717, 'eval_samples_per_second': 0.06, 'eval_steps_per_second': 0.031, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      " 81%|████████  | 25/31 [16:50<03:50, 38.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7308, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [17:17<00:11, 11.01s/it]***** Running Evaluation *****\n",
      "  Num examples = 25\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7998, 'learning_rate': 1.6129032258064516e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 97%|█████████▋| 30/31 [23:26<00:11, 11.01s/it]Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.640104055404663, 'eval_rouge2_precision': 0.1284, 'eval_rouge2_recall': 0.1252, 'eval_rouge2_fmeasure': 0.1247, 'eval_runtime': 369.0262, 'eval_samples_per_second': 0.068, 'eval_steps_per_second': 0.035, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "100%|██████████| 31/31 [23:58<00:00, 128.01s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 31/31 [23:58<00:00, 46.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1438.9551, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.022, 'train_loss': 2.9066912820262294, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31, training_loss=2.9066912820262294, metrics={'train_runtime': 1438.9551, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.022, 'train_loss': 2.9066912820262294, 'epoch': 0.99})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
